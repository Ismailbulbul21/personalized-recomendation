{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "cs5tnobjA9pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 1: Load Data\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/data.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "tBGhVmI2BBb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Missing Values by Numbers and by Graph"
      ],
      "metadata": {
        "id": "40QhlWnbBR-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style for the plot\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Check missing values by numbers\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "# Plot missing values with updated style and color map\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(data.isnull(), cbar=False, cmap='coolwarm', linewidths=0.5, linecolor='none')\n",
        "plt.title('Missing Values Heatmap', fontsize=18)\n",
        "plt.xlabel('Columns', fontsize=14)\n",
        "plt.ylabel('Rows', fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m1iTZ1bqBYzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Matrix Factorization Techniques"
      ],
      "metadata": {
        "id": "68qrQMxdBvKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 3: Use Matrix Factorization Techniques\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Handle missing values (dropping rows with missing CustomerID)\n",
        "data.dropna(subset=['CustomerID'], inplace=True)\n",
        "\n",
        "# Aggregate data by summing quantities for each user-product pair\n",
        "aggregated_data = data.groupby(['CustomerID', 'StockCode'])['Quantity'].sum().reset_index()\n",
        "\n",
        "# Create a user-item interaction matrix\n",
        "interaction_matrix = aggregated_data.pivot(index='CustomerID', columns='StockCode', values='Quantity').fillna(0)\n",
        "\n",
        "# Convert the interaction matrix to a sparse matrix format\n",
        "interaction_matrix_sparse = csr_matrix(interaction_matrix.values)\n",
        "\n",
        "# Perform Singular Value Decomposition (SVD)\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "decomposed_matrix = svd.fit_transform(interaction_matrix_sparse)\n",
        "\n",
        "# Calculate the similarity matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(decomposed_matrix)\n",
        "\n",
        "# Create a DataFrame for the similarity matrix\n",
        "similarity_df = pd.DataFrame(similarity_matrix, index=interaction_matrix.index, columns=interaction_matrix.index)\n",
        "\n",
        "print('Matrix Factorization Completed')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQrvqLeOBw0w",
        "outputId": "96828b06-8a07-40c8-9bc9-f883c0951b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Factorization Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data into Training, Validation, and Testing Sets"
      ],
      "metadata": {
        "id": "pNcN3T7HB8kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training (70%) and temp (30%)\n",
        "train_data, temp_data = train_test_split(interaction_matrix_sparse, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split temp data into validation (20% of original) and test sets (10% of original)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=1/3, random_state=42)\n",
        "\n",
        "print('Data Splitting Completed')\n"
      ],
      "metadata": {
        "id": "fzVXYWeiCCIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "iOPlHaT-CSwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "svd.fit(train_data)\n",
        "\n",
        "print('Model Training Completed')\n"
      ],
      "metadata": {
        "id": "JOKpBqX6CUjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "jdMngYKuCi4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Predict on the validation set\n",
        "val_decomposed = svd.transform(val_data)\n",
        "val_reconstructed = svd.inverse_transform(val_decomposed)\n",
        "\n",
        "# Flatten the matrices for validation\n",
        "val_data_flat = val_data.toarray().flatten()\n",
        "val_reconstructed_flat = val_reconstructed.flatten()\n",
        "\n",
        "# Calculate RMSE and MAE for validation set\n",
        "rmse_val = np.sqrt(mean_squared_error(val_data_flat, val_reconstructed_flat))\n",
        "mae_val = mean_absolute_error(val_data_flat, val_reconstructed_flat)\n",
        "\n",
        "# Predict on the test set\n",
        "test_decomposed = svd.transform(test_data)\n",
        "test_reconstructed = svd.inverse_transform(test_decomposed)\n",
        "\n",
        "# Flatten the matrices for test set\n",
        "test_data_flat = test_data.toarray().flatten()\n",
        "test_reconstructed_flat = test_reconstructed.flatten()\n",
        "\n",
        "# Calculate RMSE and MAE for test set\n",
        "rmse_test = np.sqrt(mean_squared_error(test_data_flat, test_reconstructed_flat))\n",
        "mae_test = mean_absolute_error(test_data_flat, test_reconstructed_flat)\n",
        "\n",
        "# Function to evaluate Precision@k and Recall@k\n",
        "def precision_recall_at_k(predictions, k=10):\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        n_rel = sum((true_r >= 4) for (_, true_r) in user_ratings)\n",
        "        n_rec_k = sum((est >= 4) for (est, _) in user_ratings[:k])\n",
        "        n_rel_and_rec_k = sum(((true_r >= 4) and (est >= 4)) for (est, true_r) in user_ratings[:k])\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
        "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
        "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
        "    return precision, recall\n",
        "\n",
        "# Example predictions for evaluation\n",
        "predictions = [\n",
        "    (1, 101, 4.0, 4.5),  # (user_id, item_id, actual_rating, predicted_rating)\n",
        "    (1, 102, 3.0, 2.5),\n",
        "    (2, 101, 5.0, 4.8),\n",
        "    (2, 103, 2.0, 2.2)\n",
        "]\n",
        "\n",
        "# Evaluate predictions\n",
        "precision, recall = precision_recall_at_k(predictions, k=10)\n",
        "\n",
        "# Display evaluation results\n",
        "precision_percentage = precision * 100\n",
        "recall_percentage = recall * 100\n",
        "\n",
        "print(f\"Validation RMSE: {rmse_val}\")\n",
        "print(f\"Validation MAE: {mae_val}\")\n",
        "print(f\"Test RMSE: {rmse_test}\")\n",
        "print(f\"Test MAE: {mae_test}\")\n",
        "print(f\"Precision@10: {precision_percentage}%\")\n",
        "print(f\"Recall@10: {recall_percentage}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3k06wwbClvx",
        "outputId": "ebb9d719-5700-4f46-cafb-a34d4a0d681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 9.326556163422973\n",
            "Validation MAE: 0.5342332231609958\n",
            "Test RMSE: 4.516174548228661\n",
            "Test MAE: 0.4375441991012082\n",
            "Precision@10: 100.0%\n",
            "Recall@10: 100.0%\n"
          ]
        }
      ]
    }
  ]
}